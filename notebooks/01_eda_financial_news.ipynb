{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306b01e7",
   "metadata": {},
   "source": [
    "# Major Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06000d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756ccbb",
   "metadata": {},
   "source": [
    "# --- 1. Load Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c584d49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Adjust path if your CSV is elsewhere\n",
    "    news_df = pd.read_csv('../data/raw/financial_news.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: financial_news.csv not found. Please ensure it's in the 'data/raw' directory.\")\n",
    "\n",
    "\n",
    "print(\"--- Data Understanding ---\")\n",
    "print(\"Shape:\", news_df.shape)\n",
    "print(\"\\nInfo:\")\n",
    "news_df.info()\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(news_df.head())\n",
    "print(\"\\nMissing values:\")\n",
    "print(news_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f64795",
   "metadata": {},
   "source": [
    "# --- 2. Data Cleaning & Preprocessing (Basic) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd39307",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'date' to datetime objects\n",
    "# The format includes timezone offset, pandas handles this well\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce') # errors='coerce' will turn unparseable dates into NaT\n",
    "\n",
    "# Drop rows where date conversion failed (if any)\n",
    "news_df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Ensure 'headline' is string\n",
    "news_df['headline'] = news_df['headline'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a34db",
   "metadata": {},
   "source": [
    "# --- 3. Descriptive Statistics ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e61e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Descriptive Statistics ---\")\n",
    "# Headline length\n",
    "news_df['headline_length'] = news_df['headline'].apply(len)\n",
    "print(\"\\nHeadline Length Stats:\")\n",
    "print(news_df['headline_length'].describe())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(news_df['headline_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Headline Lengths')\n",
    "plt.xlabel('Headline Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Articles per publisher\n",
    "publisher_counts = news_df['publisher'].value_counts().nlargest(20) # Top 20\n",
    "print(\"\\nTop 20 Most Active Publishers:\")\n",
    "print(publisher_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=publisher_counts.values, y=publisher_counts.index, palette='viridis')\n",
    "plt.title('Number of Articles per Publisher (Top 20)')\n",
    "plt.xlabel('Number of Articles')\n",
    "plt.ylabel('Publisher')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Publication dates trends\n",
    "news_df.set_index('date', inplace=True) # Set date as index for time series analysis\n",
    "articles_per_day = news_df['headline'].resample('D').count()\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "articles_per_day.plot()\n",
    "plt.title('Number of Articles Published Over Time (Daily)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()\n",
    "\n",
    "# Day of week analysis\n",
    "news_df['day_of_week'] = news_df.index.day_name()\n",
    "day_of_week_counts = news_df['day_of_week'].value_counts().reindex([\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=day_of_week_counts.index, y=day_of_week_counts.values)\n",
    "plt.title('Number of Articles by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()\n",
    "\n",
    "# Reset index if needed for further non-time-series operations, or keep it if mainly doing time-based\n",
    "news_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedf973",
   "metadata": {},
   "source": [
    "\n",
    "# --- 4. Text Analysis (Basic Keywords/Topic Modeling Idea) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b540364",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Basic Text Analysis ---\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Simple keyword extraction from headlines\n",
    "all_headlines_text = ' '.join(news_df['headline'].str.lower())\n",
    "words = word_tokenize(all_headlines_text)\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "word_counts = Counter(filtered_words)\n",
    "print(\"\\nMost common words in headlines (Top 20):\")\n",
    "print(word_counts.most_common(20))\n",
    "\n",
    "# Using CountVectorizer for N-grams (e.g., bi-grams)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english', max_features=20) # Top 20 bigrams\n",
    "bigrams = vectorizer.fit_transform(news_df['headline'])\n",
    "bigram_counts = pd.DataFrame({\n",
    "    'bigram': vectorizer.get_feature_names_out(),\n",
    "    'count': bigrams.sum(axis=0).A1 # .A1 converts matrix to 1D array\n",
    "}).sort_values('count', ascending=False)\n",
    "print(\"\\nMost common bigrams (Top 20):\")\n",
    "print(bigram_counts)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
